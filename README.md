# Trading-Service-Backtesting-VaR-service-

## README Content Structure

This README is organized to help contributors and maintainers quickly find the project's purpose, architecture, and how to run or extend the service.

- Overview: short project summary and design goals
- High-level components: description of core subsystems
- Suggested tech stack: recommended tools, languages and libraries
- Project layout: opinionated directory structure and key files
- Getting started: build, run, test, and Docker instructions (TBD)
- Development notes: coding standards, testing, CI, and deployment (TBD)

## Overview

Real-time trading engine that can attach strategies (AI models or rule-based).

Offline backtesting engine to validate strategies on historical data.

VaR service to estimate portfolio risk (historical, parametric, Monte Carlo).

Safe separation between simulated/backtest logic and live execution.

Observable, testable, and deployable (Docker + CI).

## High-level components

- MarketData ingestion (streaming & historical store)
- Strategy (AI model or rule-based)
- TradingEngine (live execution loop, risk checks)
- OrderService (broker adapter / mock)
- Backtester (historical replay & performance metrics)
- VaRService (historical, parametric, Monte Carlo)
- Persistence (Postgres for orders/logs, time-series DB optional)
- API (REST/gRPC for status and control)
- Monitoring (Prometheus/Grafana, logs)

## Trading Engine (Live/Simulation)

The Trading Engine consumes signals generated by strategies or AI models, applies risk controls, routes orders to a broker/exchange or simulated broker, and updates portfolio state, realized PnL, and VaR metrics.

### Architecture

```
Signal -> TradingEngine
          ├─ RiskManager (pre-trade checks)
          ├─ Broker (live or simulated)
          ├─ Portfolio (positions, cash, average cost)
          └─ ExecutionLogger (trades, metrics, state)
```

Packages:
```
com.trading.service.engine              # TradingEngine + core interfaces
com.trading.service.engine.impl         # Simple implementations (risk, broker, logger)
com.trading.service.model               # Order, Portfolio, Signal, OHLC, etc.
com.trading.service.risk                # VaRService (historical/parametric/MC)
```

### Components

- TradingEngine: Orchestrates the end-to-end execution flow. Computes VaR from recent returns, runs risk checks, submits orders, updates portfolio, and logs metrics.
- RiskManager: Approves/rejects orders based on rules (e.g., max notional, VaR limit, position/cash rules).
- Broker: Abstraction over execution. The `SimpleBroker` fills immediately at the requested price for simulation.
- ExecutionLogger: Hook for logs/metrics. `StdOutExecutionLogger` prints to console; swap with SLF4J/Micrometer in production.
- Portfolio: Tracks cash, positions, and average costs; updates on filled orders.
- VaRService: Provides historical, parametric, and Monte Carlo VaR utilities for pre-trade checks.

### Typical Flow

1. Receive Signal from model/strategy.
2. Convert to an actionable Order candidate (BUY/SELL, qty, price).
3. Compute VaR on recent returns and current portfolio value.
4. RiskManager approves or rejects.
5. Submit to Broker; on fill, update Portfolio and realized PnL.
6. Log the trade, new portfolio state, and metrics (e.g., VaR).

### Usage Example

```java
import com.trading.service.engine.TradingEngine;
import com.trading.service.engine.impl.SimpleBroker;
import com.trading.service.engine.impl.SimpleRiskManager;
import com.trading.service.engine.impl.StdOutExecutionLogger;
import com.trading.service.model.Portfolio;
import com.trading.service.model.Signal;
import java.util.*;

Portfolio portfolio = new Portfolio(10_000);
TradingEngine engine = new TradingEngine(
    portfolio,
    new SimpleBroker(),
    new SimpleRiskManager(/*maxNotional*/ 50_000, /*maxVaR*/ 10_000),
    new StdOutExecutionLogger());

Map<String, Double> prices = new HashMap<>();
prices.put("AAPL", 150.0);
List<Double> recentReturns = Arrays.asList(0.01, -0.003, 0.002, -0.004, 0.001);

Signal buy = new Signal(Signal.SignalType.BUY, 0.9, "model", "1", System.currentTimeMillis());
boolean executed = engine.processSignal("AAPL", buy, 50, 150.0, recentReturns, 0.95, prices);
```

### Realized PnL and VaR

- Realized PnL is updated on SELL fills using the average cost basis at the time of trade: `(fillPrice - avgCostBefore) * filledQty`.
- `getLastComputedVaR()` exposes the last VaR estimate used for risk approval.

### Testing

Unit test `TradingEngineTest` covers a buy then sell scenario, validating risk approval, portfolio cash updates, and realized PnL calculation:

```bash
./gradlew test --tests "*TradingEngineTest"
```

### Extending the Engine

- Risk Rules: Add max position per symbol, leverage caps, cash checks, or exposure by sector/instrument.
- Broker Integrations: Implement real exchange/broker APIs (REST/FIX/WebSocket) behind the `Broker` interface.
- Metrics: Replace `StdOutExecutionLogger` with SLF4J + Micrometer; export Prometheus metrics.
- Persistence: Append trades and positions to CSV or a relational DB; add reconciliation jobs.
- Live Wiring: Connect to the Tick Event Bus or strategy outputs to drive `processSignal` in real time.


## Real-time Market Data Ingestion (NEW)

The project now includes an event-driven Market Data Ingestion pipeline designed to normalize incoming tick/bar data and fan out to multiple sinks (Kafka, local time-series CSV, archival batch files).

### Architecture Snapshot

Raw Feed → Connector(s) (WebSocket / REST polling / FIX stub, each optionally `RawMessageCapable`) →
    IngestionGateway (JSON parse & normalize) → IngressDispatcher (bounded queue) →
        1. Kafka (ticks & bars topics) via `MarketDataPublisher`
        2. Time-series writer (CSV prototype; pluggable for real DB)
        3. Batch archiver (NDJSON for cold storage)
        4. (Optional) Tick Event Bus abstraction (`TickEventBus`) for downstream consumers / strategy engines

Key packages:
```
com.trading.service.data.ingestion.config        # IngestionConfig
com.trading.service.data.ingestion.model         # MarketDataEvent, BarEvent
com.trading.service.data.ingestion.connector     # Connectors (WebSocket, REST, FIX stub)
com.trading.service.data.ingestion.publish       # Publisher (Kafka), IngressDispatcher
com.trading.service.data.ingestion.timeseries    # TimeSeriesWriter abstraction + CSV impl
com.trading.service.data.ingestion.archive       # BatchArchiver abstraction + local impl
com.trading.service.data.ingestion.service       # MarketDataIngestionService orchestrator
```

### Enabling the Ingestion Service

The ingestion pipeline is disabled by default. Enable it by setting an environment variable before running the application:

```bash
ENABLE_INGESTION=true ./gradlew run
```

Or with Java directly:
```bash
ENABLE_INGESTION=true java -jar build/libs/<your-app>.jar
```

On startup with ingestion enabled you will see:
```
Trading Service with AI, Backtesting, VaR & Ingestion
Ingestion service started. Press Ctrl+C to exit.
```

Generated output directories (default):
```
data-output/
    ticks.csv               # Append-only CSV of tick events
    bars.csv                # Append-only CSV of bar events
    archive/
        archive.ndjson        # Batched archival lines (placeholder format)
```

### Configuration (IngestionConfig)

`IngestionConfig` uses a builder with sensible defaults. Override fields if needed:

| Setting | Default | Purpose |
|---------|---------|---------|
| kafkaBootstrapServers | `localhost:9092` | Kafka cluster bootstrap (comma-separated) |
| tickTopic | `market-data-ticks` | Topic for raw/normalized tick events |
| barTopic  | `market-data-bars`  | Topic for aggregated bars |
| restPollInterval | 1s | Interval for REST polling connector |
| dispatcherQueueCapacity | 50000 | Bounded queue size before backpressure (dropping via offer fail) |
| archiverBatchSize | 10000 | Flush threshold for batch archiver |

Example manual construction:
```java
IngestionConfig cfg = IngestionConfig.builder()
        .kafkaBootstrapServers("redpanda:9092")
        .tickTopic("ticks.demo")
        .barTopic("bars.1m.demo")
        .dispatcherQueueCapacity(100_000)
        .build();
```

### Adding Custom Connectors

Implement `MarketDataConnector`:
```java
public final class MyFeedConnector implements MarketDataConnector {
    // setTickHandler(handler) & setBarHandler(handler) will be called by the service
    public void start() { /* open socket / schedule polling */ }
    public boolean isRunning() { return true; }
    public void close() { /* cleanup */ }
    public String name() { return "my-feed"; }
}
```
Register with the ingestion service:
```java
MarketDataIngestionService service = ...;
service.registerConnector(new MyFeedConnector());
service.startAll();
```

### Kafka Topics & Serialization

Currently events are serialized as JSON strings (Jackson) directly in `MarketDataPublisher`. For production quality:
1. Introduce schema registry + Avro / Protobuf / JSON Schema.
2. Add explicit version field to events.
3. Add compression (producer config: `compression.type=zstd` or `lz4`).
4. Consider partition strategy (symbol-based hashing for ordering per instrument).

### Time-Series Storage Options

The provided `FileCsvTimeSeriesWriter` is a minimal placeholder. To integrate a real database:
1. Implement `TimeSeriesWriter` for TimescaleDB / QuestDB / InfluxDB.
2. Batch inserts (JDBC batch or line protocol) to reduce write amplification.
3. Add retention & compaction policy on the database side.

### Archival Strategy

`LocalFileBatchArchiver` accumulates a list in memory until `archiverBatchSize` then appends newline-delimited entries. Production replacements:
 - Stream directly to object storage (S3 / MinIO) using multipart uploads.
 - Include gzip compression.
 - Partition by date (e.g., `s3://bucket/marketdata/YYYY/MM/DD/`).

### Backpressure & Reliability

Current dispatcher uses an `ArrayBlockingQueue` with non-blocking `offer`. When saturated events are silently dropped (implicit lossy mode). For stronger guarantees:
1. Switch to blocking `put` and apply per-producer timeouts.
2. Add metrics (dropped events, queue depth) via Micrometer.
3. Use a ring buffer / Disruptor if ultra-low latency becomes necessary.

### Graceful Shutdown

Shutdown hook triggers:
1. Connector `close()`
2. Dispatcher drain + publisher close
3. Writer flush & archiver flush

### Local Development Without Kafka

If Kafka isn’t running, the `MarketDataPublisher` will still create a producer; sends will attempt DNS/connection. For local experimentation without Kafka:
1. Run a lightweight broker (Redpanda) via Docker:
     ```bash
     docker run -it --rm -p 9092:9092 -e REDPANDA_AUTO_CREATE_TOPICS=1 docker.redpanda.com/redpanda/redpanda:latest redpanda start --overprovisioned --smp 1 --memory 512M --reserve-memory 0M --check=false
     ```
2. Or implement a `NoopPublisher` (simple class implementing `Publisher` that does nothing) and inject it for tests.

### Sample Minimal Startup Code
```java
IngestionConfig cfg = IngestionConfig.builder().build();
MarketDataPublisher publisher = new MarketDataPublisher(cfg);
FileCsvTimeSeriesWriter writer = new FileCsvTimeSeriesWriter(Path.of("data-output"));
LocalFileBatchArchiver archiver = new LocalFileBatchArchiver(cfg, Path.of("data-output/archive"));
MarketDataIngestionService service = new MarketDataIngestionService(cfg, publisher, writer, archiver);
service.registerConnector(new RestPollingMarketDataConnector(URI.create("https://example.com/marketdata"), Duration.ofSeconds(5), Executors.newScheduledThreadPool(1)));
service.startAll();
```

### Testing

Unit test example: `IngestionDispatcherTest` validates that tick & bar submissions fan out to all sinks using stub implementations (no Kafka needed). To extend coverage:
1. Add integration test with embedded Kafka or Testcontainers.
2. Add performance test measuring max sustainable ticks/sec.

Run only ingestion tests:
```bash
./gradlew test --tests "*IngestionDispatcherTest"
```

### CI & Local `act` Note

Artifact uploads are skipped automatically when running under `act` (no `ACTIONS_RUNTIME_TOKEN`). This keeps local dry-runs green. Real CI (GitHub hosted) still uploads build & test artifacts.

### Roadmap (Proposed)
| Item | Status | Notes |
|------|--------|-------|
| Kafka JSON publishing | Done | Basic JSON serialization |
| Ingestion Gateway (raw -> normalized) | Done | Centralizes parsing & validation |
| RawMessageCapable connectors | Done | WebSocket/REST/FIX stubs forward raw payloads |
| Pluggable serialization (Avro/Protobuf) | TODO | Schema registry integration |
| Real connector parsers | TODO | Replace placeholder parsing logic |
| Metrics & monitoring | TODO | Micrometer counters & timers |
| Backpressure metrics | TODO | Dropped event counter |
| Embedded Kafka integration test | TODO | Testcontainers |
| S3/MinIO batch archiver | TODO | Multipart uploads + compression |
| Time-series DB writer | TODO | JDBC batches / line protocol |
| Replay & recovery | TODO | Offset + archival replay tooling |
| Tick Event Bus abstraction | Done | `TickEventBus` + Kafka & Pulsar (stub) implementations |
| Pulsar integration (real client) | TODO | Add Pulsar client dependency & config |
| Event bus / publisher unification | TODO | Avoid dual JSON serialization paths |

---

### Ingestion Gateway Layer

The `IngestionGateway` is a lightweight parsing & normalization layer inserted between raw connectors and the dispatcher. Responsibilities:

1. Accept raw textual payloads (currently JSON strings)
2. Parse into `MarketDataEvent` (tick) or `BarEvent` (OHLCV) via simple field heuristics
3. Perform basic validation (required fields like `symbol` & OHLC set)
4. Forward normalized objects into the `MarketDataIngestionService` (which enqueues them for fan-out)
5. Maintain simple counters for parsed vs dropped messages (future: expose via metrics)

Flow with gateway:

```
Connector (RawMessageCapable) -> rawConsumer.accept(payload)
  -> IngestionGateway.onRaw(payload)
     -> classify (tick|bar)
     -> toTick / toBar -> service.submitTick/Bar -> IngressDispatcher
        -> Kafka / TimeSeries / Archiver / (optionally) TickEventBus
```

Code snippet (from `Application`):

```java
IngestionGateway gateway = new IngestionGateway(service);
service.attachGateway(gateway); // retrofits existing connectors
```

Adding a new raw-capable connector:

```java
public final class MyJsonSocketConnector extends AbstractReconnectableConnector implements RawMessageCapable {
    private Consumer<String> raw = s -> {};
    @Override public void setRawMessageConsumer(Consumer<String> c) { this.raw = c; }
    // inside onText/onMessage:
    // raw.accept(incomingPayload);
}
```

### Tick Event Bus Abstraction

To decouple downstream consumers (e.g., strategy engines, analytics) from the ingestion publishing mechanics, a `TickEventBus` interface was introduced:

```java
public interface TickEventBus extends AutoCloseable {
    void publish(MarketDataEvent event);
    void close();
}
```

Implementations provided:

| Implementation | Class | Status | Notes |
|----------------|-------|--------|-------|
| Kafka | `KafkaTickEventBus` | Ready | Mirror of existing publisher logic for ticks only |
| Pulsar | `PulsarTickEventBus` | Stub | Placeholder; serialize JSON asynchronously (no real client yet) |

Planned next steps:
1. Optionally replace direct tick publishing in `IngressDispatcher` with a `TickEventBus` instance (reducing duplicate JSON serialization logic)
2. Extend bus abstraction for bar events or unify via generic parameter / sealed hierarchy
3. Wire bus selection via `IngestionConfig` (enum: KAFKA, PULSAR, NOOP)

Example usage with a capturing in-memory bus (testing pattern):

```java
TickEventBus bus = new TickEventBus() {
    final AtomicInteger count = new AtomicInteger();
    public void publish(MarketDataEvent e) { count.incrementAndGet(); }
    public void close() {}
};
bus.publish(new MarketDataEvent("TEST", System.currentTimeMillis(), 1,2,1.5,10));
```

### Why Gateway + Bus?

| Concern | Gateway | Tick Event Bus |
|---------|---------|----------------|
| Parsing/Normalization | YES | NO |
| Backpressure (queue) | Indirect (enqueues) | Depends on implementation |
| Multi-sink fan-out | Indirect (dispatcher) | Out of scope (single channel) |
| Extensibility (Pulsar, gRPC, WebSocket push) | Via connectors | Via new bus impls |
| Metrics surface | Will expose counters | Per-bus metrics (publish latency, errors) |

### Future Enhancements

1. Replace heuristic JSON parsing with schema-based approach (Schema Registry, Protobuf, Avro)
2. Introduce structured error logging / dead-letter path for malformed messages
3. Expose gateway & dispatcher metrics via Micrometer
4. Apply rate limiting / adaptive backpressure at gateway level
5. Integrate a real Pulsar producer & optional multi-broker replication


---

## Neural Translation System

### Overview

The trading service includes a sophisticated multi-language translation system powered by Facebook's M2M100 transformer model, capable of translating trading content across 100+ languages while preserving financial terminology and context.

### Key Features

- **Multi-Language Support**: Translate between 100+ languages using state-of-the-art M2M100 transformer
- **Trading-Specific Terminology**: Preserves financial and trading terms during translation
- **Batch Processing**: Efficient handling of multiple translation requests
- **Graceful Fallback**: Works with or without ML dependencies installed
- **Java Integration**: Seamless integration with existing Java codebase
- **Caching Support**: Optional translation caching for improved performance

### Architecture

The translation system consists of several components:

1. **Python ML Backend** (`language_translation_models.py`)
   - Core M2M100 model implementation
   - Trading terminology preservation
   - Batch processing capabilities
   - PyTorch and Hugging Face integration

2. **Java Integration Layer** 
   - `SimpleTranslationService.java` - Lightweight integration (no external dependencies)
   - `TranslationService.java` - Full-featured integration with caching and batch support

3. **CLI Interface** (`translation_cli.py`)
   - Command-line interface for Java-Python communication
   - JSON-based request/response handling

4. **Configuration & Utilities**
   - `translation_config.py` - Configuration management
   - `translation_utils.py` - Helper functions and utilities
   - `requirements.txt` - Python dependencies

### Quick Start

#### Option 1: Simple Integration (No ML Dependencies Required)

```java
import com.trading.service.model.TransformerLanguageTranslation.SimpleTranslationService;

SimpleTranslationService translator = new SimpleTranslationService();
String result = translator.translateText("Buy 100 shares of AAPL", "en", "es");
System.out.println(result); // Returns translated text or graceful fallback
```

#### Option 2: Full ML-Powered Translation

1. Install Python dependencies:
```bash
pip install -r src/main/java/com/trading/service/model/TransformerLanguageTranslation/requirements.txt
```

2. Use the full-featured service:
```java
import com.trading.service.model.TransformerLanguageTranslation.TranslationService;

TranslationService translator = new TranslationService();
TranslationResponse response = translator.translateText("Market volatility increased", "en", "fr");
System.out.println(response.getTranslatedText());
```

### Supported Languages

The M2M100 model supports translation between major languages including:
- English (en), Spanish (es), French (fr), German (de)
- Chinese (zh), Japanese (ja), Korean (ko)
- Arabic (ar), Russian (ru), Portuguese (pt)
- Italian (it), Dutch (nl), Polish (pl)
- And 90+ additional languages

### Configuration

Translation behavior can be customized through `translation_config.py`:

```python
TRANSLATION_CONFIG = {
    "model_name": "facebook/m2m100_418M",  # Model size variant
    "max_length": 512,                    # Maximum translation length
    "num_beams": 5,                       # Beam search parameter
    "preserve_trading_terms": True,       # Preserve financial terminology
    "batch_size": 10,                     # Batch processing size
    "cache_translations": True            # Enable translation caching
}
```

### Trading Terminology Preservation

The system includes specialized logic to preserve trading-specific terms:

- **Financial Instruments**: AAPL, MSFT, BTC, ETH, USD, EUR
- **Trading Actions**: Buy, Sell, Hold, Long, Short
- **Market Terms**: Volatility, Liquidity, Support, Resistance
- **Technical Indicators**: RSI, MACD, Bollinger Bands

### Testing

The translation system includes comprehensive tests:

```bash
# Run all tests including translation components
./gradlew test

# Test specific translation functionality
./gradlew test --tests "*Translation*"
```

### Performance Considerations

- **Model Loading**: Initial model load takes 10-30 seconds depending on hardware
- **Translation Speed**: ~1-5 seconds per sentence depending on length and complexity
- **Memory Usage**: Requires ~2-4GB RAM for M2M100 model in memory
- **Caching**: Enables sub-second response times for repeated translations

### Troubleshooting

**Common Issues:**

1. **Python Dependencies Missing**: The system gracefully falls back to placeholder text
2. **Out of Memory**: Use smaller model variant (e.g., `facebook/m2m100_418M`)
3. **Slow Performance**: Enable caching and consider batch processing
4. **Language Not Supported**: Check M2M100 language codes in documentation

**Debug Mode:**

Enable detailed logging by setting environment variable:
```bash
export TRANSLATION_DEBUG=true
```

### Files and Structure

```
src/main/java/com/trading/service/model/TransformerLanguageTranslation/
├── SimpleTranslationService.java     # Lightweight Java integration
├── TranslationService.java           # Full-featured Java integration
├── language_translation_models.py    # Core M2M100 implementation
├── translation_cli.py               # Python CLI interface
├── translation_config.py            # Configuration settings
├── translation_utils.py             # Utility functions
├── requirements.txt                 # Python dependencies
├── README_Translation.md            # Detailed documentation
└── example_usage.py                # Usage examples
```

## Backtesting System with VaR Integration

### Overview

The trading service includes a comprehensive backtesting system that implements the complete workflow for historical strategy evaluation with integrated Value-at-Risk (VaR) analysis. The system supports multi-asset backtesting with realistic order execution simulation and rolling VaR calculations.

### Core Workflow

The backtesting system implements the following complete workflow:

1. **Load Historical Price Series**: Load OHLC data for multiple assets (A, B, etc.) from CSV files or generate synthetic data
2. **Calculate Daily Returns**: Compute both simple and log returns for statistical analysis
3. **Strategy Execution**: Feed OHLC bars to trading strategies and generate realistic order fills with slippage
4. **Portfolio Management**: Track daily P&L, update portfolio positions, and maintain cash balances
5. **Rolling VaR Analysis**: Calculate VaR using last N returns with configurable rolling window
6. **Comprehensive Reporting**: Generate detailed backtest reports including VaR metrics and performance statistics

### Key Components

#### Data Models

**OHLC (Open, High, Low, Close)**
```java
// Represents price data for a specific time period
OHLC ohlc = new OHLC("AAPL", LocalDate.of(2023, 1, 1), 150.0, 152.5, 149.0, 151.0, 1000000);
double dailyReturn = ohlc.getDailyReturn(previousClose);
double logReturn = ohlc.getLogReturn(previousClose);
```

**Portfolio Management**
```java
// Initialize portfolio with cash
Portfolio portfolio = new Portfolio(100000.0); // $100,000

// Execute orders
Order buyOrder = new Order("BUY1", "AAPL", Order.OrderType.BUY, 100, 150.0, LocalDate.now());
buyOrder.fill(100, 150.50, LocalDate.now()); // Realistic fill with slippage
portfolio.executeOrder(buyOrder);

// Calculate portfolio value
Map<String, Double> currentPrices = Map.of("AAPL", 155.0);
double portfolioValue = portfolio.getPortfolioValue(currentPrices);
```

#### Market Data Loading

**MarketDataLoader**
```java
MarketDataLoader loader = new MarketDataLoader();

// Load from CSV file (format: Date,Open,High,Low,Close,Volume)
List<OHLC> historicalData = loader.loadHistoricalData("AAPL", "path/to/aapl.csv");

// Generate synthetic data for testing
List<OHLC> sampleData = loader.generateSampleData("TEST", LocalDate.now().minusDays(100), 100, 100.0);

// Calculate returns
List<Double> dailyReturns = loader.calculateDailyReturns(historicalData);
List<Double> logReturns = loader.calculateLogReturns(historicalData);

// Filter by date range
List<OHLC> filtered = loader.filterByDateRange(historicalData, startDate, endDate);
```

#### Trading Strategies

**TradingStrategy Interface**
```java
public interface TradingStrategy {
    void initialize(List<OHLC> historicalData);
    List<Order> generateSignals(OHLC currentBar, List<OHLC> historicalBars, LocalDate currentDate);
    String getStrategyName();
    void cleanup();
}
```

**Moving Average Crossover Strategy**
```java
// Create strategy: 10-day MA crosses 30-day MA, 100 shares per trade
TradingStrategy strategy = new MovingAverageCrossoverStrategy(10, 30, 100);

// Strategy generates buy/sell signals based on MA crossovers
List<Order> signals = strategy.generateSignals(currentBar, historicalBars, currentDate);
```

#### VaR Integration

**VaRService with Multiple Methods**
```java
VaRService varService = new VaRService();
double portfolioValue = 100000.0;
double confidenceLevel = 0.95; // 95% confidence
List<Double> returns = portfolioReturns.subList(portfolioReturns.size() - 30, portfolioReturns.size()); // 30-day window

// Calculate VaR using different methods
double historicalVaR = VaRService.historicalVaR(returns, confidenceLevel, portfolioValue);
double parametricVaR = VaRService.parametricVaR(returns, confidenceLevel, portfolioValue);
double monteCarloVaR = VaRService.monteCarloVaR(returns, confidenceLevel, portfolioValue, 10000);
```

**VaRReport for Daily Tracking**
```java
VaRReport dailyVaR = new VaRReport(
    LocalDate.now(),
    portfolioValue,
    historicalVaR,
    parametricVaR, 
    monteCarloVaR,
    confidenceLevel,
    windowSize
);

// Get VaR as percentage of portfolio
double historicalVaRPercent = dailyVaR.getHistoricalVaRPercent(); // e.g., 2.5%
```

### Backtesting Configuration and Execution

**BacktestConfig Setup**
```java
// Configure backtest parameters
BacktestConfig config = new BacktestConfig(
    "Multi-Asset MA Crossover with VaR",           // Description
    Arrays.asList("AAPL", "MSFT"),                 // Asset symbols
    Map.of("AAPL", "data/aapl.csv", "MSFT", "data/msft.csv"), // Data file paths
    LocalDate.of(2023, 1, 1),                     // Start date
    LocalDate.of(2023, 12, 31),                   // End date
    100000.0,                                      // Initial cash
    new MovingAverageCrossoverStrategy(10, 30, 100), // Trading strategy
    30,                                            // VaR rolling window (days)
    0.95                                           // VaR confidence level
);
```

**Running the Backtest**
```java
Backtest backtest = new Backtest();
BacktestResult result = backtest.run(config);

// Access comprehensive results
System.out.println("Total Return: " + result.getTotalReturn() + "%");
System.out.println("Max Drawdown: " + result.getMaxDrawdown() + "%");
System.out.println("Sharpe Ratio: " + result.getSharpeRatio());

// Access VaR metrics
System.out.println("Average Historical VaR: $" + result.getAverageHistoricalVaR());
System.out.println("Average Parametric VaR: $" + result.getAverageParametricVaR());
System.out.println("Average Monte Carlo VaR: $" + result.getAverageMonteCarloVaR());

// Daily VaR reports
List<VaRReport> dailyVaRReports = result.getDailyVaRReports();
for (VaRReport report : dailyVaRReports) {
    System.out.println(report); // Detailed daily VaR information
}
```

### Advanced Features

#### Realistic Order Execution
- **Slippage Simulation**: Orders filled with configurable slippage (default 0.1%)
- **Price Validation**: Fill prices constrained within daily high/low range
- **Market Impact**: Simulated market impact for large orders

#### Rolling VaR Analysis
- **Configurable Window**: 30-day default, customizable rolling window
- **Multiple Methods**: Historical, Parametric (normal distribution), Monte Carlo
- **Daily Tracking**: VaR calculated and stored for each trading day
- **Percentage Reporting**: VaR expressed both in absolute dollars and as portfolio percentage

#### Performance Metrics
- **Total Return**: (Final Value - Initial Value) / Initial Value
- **Maximum Drawdown**: Largest peak-to-trough decline
- **Sharpe Ratio**: Risk-adjusted return metric (assuming 0% risk-free rate)
- **Volatility**: Annualized portfolio volatility

### File Structure

```
src/main/java/com/trading/service/
├── backtest/
│   ├── Backtest.java                    # Main backtesting engine
│   ├── BacktestConfig.java              # Configuration parameters
│   ├── BacktestResult.java              # Comprehensive results
│   ├── VaRReport.java                   # Daily VaR reporting
│   └── BacktestExample.java             # Usage examples
├── backtesting/
│   └── MarketDataLoader.java            # Historical data loading
├── model/
│   ├── OHLC.java                        # Price data model
│   ├── Order.java                       # Order management
│   └── Portfolio.java                   # Portfolio tracking
├── strategy/
│   ├── TradingStrategy.java             # Strategy interface
│   └── MovingAverageCrossoverStrategy.java # MA crossover implementation
└── risk/
    └── VaRService.java                  # VaR calculations
```

### Testing and Validation

Run the backtesting demonstration:
```bash
# Python demonstration showing complete workflow
python3 backtesting_demo.py

# Java unit tests (when compilation issues resolved)
./gradlew test --tests "*Backtest*"
```

### Sample Output

```
=== BACKTEST RESULTS SUMMARY ===
Status: SUCCESS
Initial Value: $100,000.00
Final Value: $100,673.46
Total Return: 0.67%
Max Drawdown: 2.22%
Sharpe Ratio: 0.296
Trading Days: 252
VaR Reports Generated: 223

=== AVERAGE VAR METRICS ===
Average Historical VaR: $199.37
Average Parametric VaR: $181.18

=== SAMPLE VAR REPORTS ===
Date: 2023-09-09
  Portfolio Value: $100,673.46
  Historical VaR: $237.14 (0.24%)
  Parametric VaR: $257.41 (0.26%)
  Monte Carlo VaR: $245.68 (0.24%)
```

### Integration with Existing Components

The backtesting system seamlessly integrates with:
- **VaRService**: Existing risk management for VaR calculations
- **ONNX Models**: Strategies can use ML models for signal generation
- **Translation System**: Multi-language reporting and documentation
- **Existing Architecture**: Follows established patterns and interfaces

## Suggested tech stack

- Java 17 (mandatory)
- Build: Gradle (wrapper enabled)
- Web/API: Spring Boot (optional) or lightweight framework (Micronaut/Quarkus)
- Data: Postgres for relational; local file storage for CSV historical data; optional TimescaleDB
- Messaging: Kafka or Redis Streams (optional) for live tick/event ingestion
- ML/AI: ONNX runtime (for exported models), TensorFlow Java, or call out to Python microservice (recommended for heavy training)
- Math & stats: Apache Commons Math, Smile, or ojAlgo
- Serialization: Jackson
- Testing: JUnit 5, Mockito
- Logging: SLF4J + Logback
- Container: Docker
- CI: GitHub Actions / GitLab CI
- Metrics: Micrometer + Prometheus + Grafana

<img width="600" height="367" alt="Screenshot 2025-09-30 at 10 39 07 AM" src="https://github.com/user-attachments/assets/1976f16c-58fd-4961-8128-f0b50b98619b" />


## Getting started

Quick steps to build, run and test the project locally. Adjust the commands if your project uses Spring Boot / different jar name.

Build the project with Gradle:

```bash
./gradlew clean build
```

Run the application (if packaged as a fat/boot jar):

```bash
java -jar build/libs/<your-app>.jar
```

Alternatively (if using Spring Boot plugin):

```bash
./gradlew bootRun
```

Run tests:

```bash
./gradlew test
```

Docker (build & run):

```bash
# build the project first, then build the image
./gradlew clean build
docker build -t trading-service:latest .
# run the container, mapping port 8080
docker run -p 8080:8080 trading-service:latest
```

Notes:
- Replace `<your-app>.jar` with the actual jar filename produced under `build/libs/`.
- If you want a multi-stage Docker build that compiles inside the image, I can add that variant.

## Development notes (TBD)

- Coding style: follow project conventions
- Tests: unit and integration tests with JUnit 5
- CI: GitHub Actions for build and tests

## Formatting

This project uses Spotless with google-java-format. To format code locally run:

```bash
./gradlew :app:format
```

CI will run `./gradlew spotlessCheck` and fail if formatting is not applied.
